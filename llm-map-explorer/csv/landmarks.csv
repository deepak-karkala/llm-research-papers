id,name,type,year,organization,authors,description,abstract,externalLinks,coordinates,capabilityId,relatedLandmarks,tags,icon,metadata,zoomThreshold
lm-001,Attention Is All You Need,paper,2017,Google DeepMind,"Vaswani, A.; Shazeer, N.; Parmar, N.","Introduced the Transformer architecture with self-attention mechanisms.","This paper presents the Transformer, a novel sequence-to-sequence architecture based entirely on attention mechanisms.","[{""type"": ""arxiv"", ""url"": ""https://arxiv.org/abs/1706.03762"", ""label"": ""arXiv""}, {""type"": ""paper"", ""url"": ""https://proceedings.neurips.cc/paper/2017/file/"", ""label"": ""PDF""}]","[37.78, -122.41]",cap-001,"lm-002,lm-003","transformer,attention,sequence-to-sequence,nlp",ðŸ“„,"{}",0
lm-002,GPT-2,model,2019,OpenAI,"Radford, A.; Wu, J.; Child, R.","Large-scale unsupervised language model with 1.5B parameters.","Language models are unsupervised multitask learners. We demonstrate that language models begin learning these tasks without any explicit supervision when trained on a new dataset of Reddit links.","[{""type"": ""arxiv"", ""url"": ""https://arxiv.org/abs/1902.10165"", ""label"": ""arXiv""}, {""type"": ""github"", ""url"": ""https://github.com/openai/gpt-2"", ""label"": ""GitHub""}]","[37.79, -122.40]",cap-002,"lm-001,lm-003","language-model,generation,gpt",ðŸ¤–,"{""parameters"": ""1.5B"", ""architecture"": ""Transformer"", ""trainingMethod"": ""Unsupervised pre-training"", ""capabilities"": [""text-generation"", ""few-shot-learning""], ""releaseDate"": ""2019-02-14"", ""license"": ""MIT""}",1
lm-003,BERT,model,2018,Google Research,"Devlin, J.; Chang, M.-W.; Lee, K.","Bidirectional Encoder Representations from Transformers for understanding contextual word relationships.","We introduce BERT, a new method of pre-training language representations. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text.","[{""type"": ""arxiv"", ""url"": ""https://arxiv.org/abs/1810.04805"", ""label"": ""arXiv""}, {""type"": ""github"", ""url"": ""https://github.com/google-research/bert"", ""label"": ""GitHub""}]","[37.80, -122.39]",cap-002,"lm-001,lm-002","language-model,pretraining,bert,embeddings",ðŸ¤–,"{""parameters"": ""340M"", ""architecture"": ""Transformer"", ""trainingMethod"": ""Masked Language Modeling"", ""capabilities"": [""text-classification"", ""token-classification"", ""semantic-similarity""], ""releaseDate"": ""2018-10-11"", ""license"": ""Apache-2.0""}",1
lm-004,Constitutional AI,paper,2023,Anthropic,"Bai, Y.; Jones, A.; Ndousse, K.","Harmless and helpful AI through constitutional methods and self-critique.","Language models demonstrate remarkable capabilities across a diverse set of tasks, but their proneness to generating harmful or misleading content remains a critical concern.","[{""type"": ""arxiv"", ""url"": ""https://arxiv.org/abs/2212.08073"", ""label"": ""arXiv""}]","[37.81, -122.38]",cap-003,"lm-005","alignment,safety,constitutional-ai,harmful-content",ðŸ“„,"{}",1
lm-005,GPT-4,model,2023,OpenAI,"OpenAI Team","Multimodal large language model with improved reasoning and reduced hallucinations.","GPT-4 is a large multimodal model that, while less capable than humans in many real-world scenarios, exhibits human-level performance on various professional and academic benchmarks.","[{""type"": ""paper"", ""url"": ""https://arxiv.org/abs/2303.08774"", ""label"": ""Technical Report""}]","[37.82, -122.37]",cap-002,"lm-002,lm-004","language-model,gpt,multimodal,frontier",ðŸ¤–,"{""parameters"": ""Unknown"", ""architecture"": ""Transformer"", ""trainingMethod"": ""RLHF with constitutional AI"", ""capabilities"": [""text-generation"", ""vision"", ""code-generation"", ""reasoning""], ""releaseDate"": ""2023-03-14"", ""license"": ""Proprietary""}",0
