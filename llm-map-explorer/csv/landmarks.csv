id,name,type,year,organization,authors,description,abstract,externalLinks,coordinates,capabilityId,relatedLandmarks,tags,icon,metadata,zoomThreshold
landmark-001,Attention Is All You Need,paper,2017,Google Brain,[],"Introduced the Transformer architecture, which is the foundation for most modern large language models.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.","[{""type"":""arxiv"",""url"":""https://arxiv.org/abs/1706.03762"",""label"":""arXiv:1706.03762""}]","{""lat"":800,""lng"":1200}",attention-architecture,[],"[""attention"",""transformer"",""architecture""]",,{},-1
landmark-002,BERT,model,2018,Google,[],"Bidirectional Encoder Representations from Transformers, a powerful language representation model.",,"[{""type"":""arxiv"",""url"":""https://arxiv.org/abs/1810.04805"",""label"":""arXiv:1810.04805""}]","{""lat"":1000,""lng"":1000}",attention-architecture,[],"[""bert"",""transformer"",""representation""]",,{},-1
landmark-003,GPT-2,model,2019,OpenAI,[],A large-scale unsupervised language model that can generate coherent paragraphs of text.,,"[{""type"":""website"",""url"":""https://openai.com/blog/better-language-models/"",""label"":""OpenAI Blog""}]","{""lat"":800,""lng"":3200}",reasoning-planning,[],"[""gpt-2"",""language-model"",""openai""]",,{},0
landmark-004,GPT-3,model,2020,OpenAI,[],"Generative Pre-trained Transformer 3, an autoregressive language model with 175 billion parameters.",,"[{""type"":""arxiv"",""url"":""https://arxiv.org/abs/2005.14165"",""label"":""arXiv:2005.14165""}]","{""lat"":1000,""lng"":3000}",reasoning-planning,[],"[""gpt-3"",""language-model"",""openai""]",,{},-1
landmark-005,FLAN,model,2021,Google,[],"Finetuned Language Net, a model that improves zero-shot learning by instruction tuning.",,"[{""type"":""arxiv"",""url"":""https://arxiv.org/abs/2109.01652"",""label"":""arXiv:2109.01652""}]","{""lat"":1800,""lng"":1600}",lora-peft,[],"[""flan"",""instruction-tuning"",""google""]",,{},0
landmark-006,InstructGPT,paper,2022,OpenAI,[],"A model that is better at following instructions than GPT-3, by using reinforcement learning from human feedback.",,"[{""type"":""arxiv"",""url"":""https://arxiv.org/abs/2203.02155"",""label"":""arXiv:2203.02155""}]","{""lat"":1700,""lng"":2100}",rlhf-archipelago,[],"[""instructgpt"",""rlhf"",""openai""]",,{},-1
landmark-007,ChatGPT,model,2022,OpenAI,[],"A conversational AI model based on the GPT architecture, optimized for dialogue.",,"[{""type"":""website"",""url"":""https://openai.com/blog/chatgpt/"",""label"":""OpenAI Blog""}]","{""lat"":800,""lng"":2200}",alignment-safety,[],"[""chatgpt"",""conversational-ai"",""openai""]",,{},0
landmark-008,LLaMA,model,2023,Meta,[],"Large Language Model Meta AI, a family of large language models.",,"[{""type"":""arxiv"",""url"":""https://arxiv.org/abs/2302.13971"",""label"":""arXiv:2302.13971""}]","{""lat"":1200,""lng"":3400}",reasoning-planning,[],"[""llama"",""language-model"",""meta""]",,{},0
landmark-009,Alpaca,model,2023,Stanford,[],A model fine-tuned from LLaMA 7B on 52K instruction-following demonstrations.,,"[{""type"":""github"",""url"":""https://github.com/tatsu-lab/stanford_alpaca"",""label"":""GitHub""}]","{""lat"":1800,""lng"":1500}",lora-peft,[],"[""alpaca"",""instruction-following"",""stanford""]",,{},1
landmark-010,LoRA,paper,2021,Microsoft,[],"Low-Rank Adaptation of Large Language Models, a parameter-efficient fine-tuning method.",,"[{""type"":""arxiv"",""url"":""https://arxiv.org/abs/2106.09685"",""label"":""arXiv:2106.09685""}]","{""lat"":1700,""lng"":1600}",lora-peft,[],"[""lora"",""peft"",""microsoft""]",,{},-1
landmark-011,QLoRA,paper,2023,UW,[],"Quantized Low-Rank Adaptation, a method for fine-tuning large language models with even less memory.",,"[{""type"":""arxiv"",""url"":""https://arxiv.org/abs/2305.14314"",""label"":""arXiv:2305.14314""}]","{""lat"":1700,""lng"":1000}",quantization-techniques,[],"[""qlora"",""quantization"",""uw""]",,{},1
landmark-012,GPTQ,paper,2023,IST,[],A one-shot weight quantization method for large language models.,,"[{""type"":""arxiv"",""url"":""https://arxiv.org/abs/2210.17323"",""label"":""arXiv:2210.17323""}]","{""lat"":1800,""lng"":900}",quantization-techniques,[],"[""gptq"",""quantization"",""ist""]",,{},1
landmark-013,CLIP,model,2021,OpenAI,[],"Contrastive Language-Image Pre-training, a model that learns visual concepts from natural language supervision.",,"[{""type"":""arxiv"",""url"":""https://arxiv.org/abs/2103.00020"",""label"":""arXiv:2103.00020""}]","{""lat"":1700,""lng"":4000}",vision-language,[],"[""clip"",""multimodal"",""openai""]",,{},0
landmark-014,DALL-E,model,2021,OpenAI,[],A neural network that creates images from text captions for a wide range of concepts expressible in natural language.,,"[{""type"":""website"",""url"":""https://openai.com/blog/dall-e/"",""label"":""OpenAI Blog""}]","{""lat"":1800,""lng"":3950}",vision-language,[],"[""dall-e"",""multimodal"",""openai""]",,{},0
landmark-015,Flamingo,paper,2022,DeepMind,[],A Visual Language Model for few-shot learning.,,"[{""type"":""arxiv"",""url"":""https://arxiv.org/abs/2204.14178"",""label"":""arXiv:2204.14178""}]","{""lat"":1000,""lng"":3900}",multimodal-capabilities,[],"[""flamingo"",""multimodal"",""deepmind""]",,{},0
landmark-016,RoBERTa,model,2019,Facebook,[],A Robustly Optimized BERT Pretraining Approach.,,"[{""type"":""arxiv"",""url"":""https://arxiv.org/abs/1907.11692"",""label"":""arXiv:1907.11692""}]","{""lat"":1200,""lng"":1400}",attention-architecture,[],"[""roberta"",""bert"",""facebook""]",,{},0
landmark-017,T5,model,2019,Google,[],"Text-to-Text Transfer Transformer, a unified framework for NLP tasks.",,"[{""type"":""arxiv"",""url"":""https://arxiv.org/abs/1910.10683"",""label"":""arXiv:1910.10683""}]","{""lat"":900,""lng"":900}",attention-architecture,[],"[""t5"",""transformer"",""google""]",,{},0
landmark-018,GPT-4,model,2023,OpenAI,[],"A large-scale, multimodal model that can accept image and text inputs and produce text outputs.",,"[{""type"":""arxiv"",""url"":""https://arxiv.org/abs/2303.08774"",""label"":""arXiv:2303.08774""}]","{""lat"":800,""lng"":3000}",reasoning-planning,[],"[""gpt-4"",""multimodal"",""openai""]",,{},0
landmark-019,Mistral-7B,model,2023,Mistral AI,[],A 7-billion parameter language model that outperforms Llama 2 13B on all benchmarks.,,"[{""type"":""arxiv"",""url"":""https://arxiv.org/abs/2310.06825"",""label"":""arXiv:2310.06825""}]","{""lat"":1200,""lng"":3200}",reasoning-planning,[],"[""mistral-7b"",""language-model"",""mistral-ai""]",,{},1
landmark-020,CodeLLaMA,model,2023,Meta,[],A family of large language models for code generation.,,"[{""type"":""arxiv"",""url"":""https://arxiv.org/abs/2308.12950"",""label"":""arXiv:2308.12950""}]","{""lat"":1800,""lng"":3600}",tool-use-agents,[],"[""codellama"",""code-generation"",""meta""]",,{},1
landmark-021,PEFT,tool,2023,Hugging Face,[],Parameter-Efficient Fine-Tuning (PEFT) library.,,"[{""type"":""github"",""url"":""https://github.com/huggingface/peft"",""label"":""GitHub""}]","{""lat"":1800,""lng"":1700}",lora-peft,[],"[""peft"",""tool"",""hugging-face""]",,{},1
landmark-022,Transformer-XL,paper,2019,Google,[],A transformer architecture that can learn dependency beyond a fixed length without disrupting temporal coherence.,,"[{""type"":""arxiv"",""url"":""https://arxiv.org/abs/1901.02860"",""label"":""arXiv:1901.02860""}]","{""lat"":1100,""lng"":1200}",attention-architecture,[],"[""transformer-xl"",""architecture"",""google""]",,{},0
landmark-023,BLIP,model,2022,Salesforce,[],Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation.,,"[{""type"":""arxiv"",""url"":""https://arxiv.org/abs/2201.12086"",""label"":""arXiv:2201.12086""}]","{""lat"":1200,""lng"":3950}",multimodal-capabilities,[],"[""blip"",""multimodal"",""salesforce""]",,{},1
landmark-024,Falcon,model,2023,TII,[],A powerful open-source language model.,,"[{""type"":""website"",""url"":""https://falconllm.tii.ae/"",""label"":""Official Website""}]","{""lat"":1000,""lng"":3400}",reasoning-planning,[],"[""falcon"",""language-model"",""tii""]",,{},1
landmark-025,DPO,paper,2023,Stanford,[],Direct Preference Optimization: Your Language Model is Secretly a Reward Model.,,"[{""type"":""arxiv"",""url"":""https://arxiv.org/abs/2305.18290"",""label"":""arXiv:2305.18290""}]","{""lat"":1800,""lng"":2200}",rlhf-archipelago,[],"[""dpo"",""rlhf"",""stanford""]",,{},0
landmark-026,SFT - Supervised Fine-Tuning,tool,2021,Various,[],A common technique for adapting pre-trained language models to specific tasks.,,"[{""type"":""website"",""url"":""https://huggingface.co/docs/trl/sft_trainer"",""label"":""Hugging Face TRL""}]","{""lat"":2000,""lng"":1600}",training-optimization,[],"[""sft"",""fine-tuning"",""tool""]",,{},1