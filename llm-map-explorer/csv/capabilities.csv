id,name,description,shortDescription,level,polygonCoordinates,visualStyleHints,relatedLandmarks,parentCapabilityId,zoomThreshold
cap-001,Attention Mechanisms,"Study of attention and self-attention patterns in neural networks. Foundation of modern transformer-based architectures.","Attention & self-attention patterns",continent,"[[37.7749, -122.4194], [37.8749, -122.4194], [37.8749, -122.3194], [37.7749, -122.3194]]","{""fillColor"": ""#FF6B6B"", ""fillOpacity"": 0.4, ""strokeColor"": ""#FF0000"", ""strokeWeight"": 2}","lm-001,lm-002","",0
cap-002,Transformers,"Foundation of modern LLMs built on attention mechanisms and encoder-decoder architectures.","Transformer architectures",archipelago,"[[37.78, -122.41], [37.87, -122.41], [37.87, -122.32], [37.78, -122.32]]","{""fillColor"": ""#4ECDC4"", ""fillOpacity"": 0.4, ""strokeColor"": ""#0099CC"", ""strokeWeight"": 2}","lm-001,lm-003",cap-001,1
cap-003,Alignment & Safety,"Techniques for aligning language models with human values and ensuring safe behavior.","Model alignment & safety techniques",archipelago,"[[37.79, -122.42], [37.88, -122.42], [37.88, -122.33], [37.79, -122.33]]","{""fillColor"": ""#95E1D3"", ""fillOpacity"": 0.4, ""strokeColor"": ""#00AA00"", ""strokeWeight"": 2}","lm-004,lm-005","",1
cap-004,Language Modeling,"Core techniques for training large language models on diverse text corpora.","Language model training techniques",continent,"[[37.80, -122.43], [37.89, -122.43], [37.89, -122.34], [37.80, -122.34]]","{""fillColor"": ""#F38181"", ""fillOpacity"": 0.4, ""strokeColor"": ""#CC0000"", ""strokeWeight"": 2}","lm-002,lm-003","",-1
