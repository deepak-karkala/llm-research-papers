id,name,description,shortDescription,level,polygonCoordinates,visualStyleHints,relatedLandmarks,parentCapabilityId,zoomThreshold
attention-architecture,Attention & Architecture,"Foundational transformer architectures and attention mechanisms that power modern LLMs. Includes self-attention, multi-head attention, and architectural innovations like sparse attention and efficient transformers. These techniques form the backbone of nearly all state-of-the-art language models.",Transformer foundations and attention mechanisms,continent,"[{""lat"":600,""lng"":800},{""lat"":600,""lng"":1600},{""lat"":1400,""lng"":1600},{""lat"":1400,""lng"":800}]","{""fillColor"":""#1976d2"",""fillOpacity"":0.45,""strokeColor"":""#1565c0"",""strokeWeight"":2,""pattern"":""solid""}",[],,-1
alignment-safety,Alignment & Safety,"Research focused on aligning language models with human values and ensuring safe AI behavior. Encompasses RLHF, constitutional AI, value learning, and safety frameworks. Critical for developing AI systems that are helpful, harmless, and honest.",Model alignment and safety research,continent,"[{""lat"":600,""lng"":1800},{""lat"":600,""lng"":2600},{""lat"":1400,""lng"":2600},{""lat"":1400,""lng"":1800}]","{""fillColor"":""#4caf50"",""fillOpacity"":0.45,""strokeColor"":""#388e3c"",""strokeWeight"":2,""pattern"":""solid""}",[],,-1
reasoning-planning,Reasoning & Planning,"Advanced reasoning techniques enabling LLMs to decompose complex problems and plan solutions. Includes chain-of-thought prompting, tree-of-thought search, tool use, and agent frameworks. Essential for tackling multi-step reasoning tasks and complex decision-making.",Complex reasoning and multi-step planning,continent,"[{""lat"":600,""lng"":2800},{""lat"":600,""lng"":3600},{""lat"":1400,""lng"":3600},{""lat"":1400,""lng"":2800}]","{""fillColor"":""#2196f3"",""fillOpacity"":0.45,""strokeColor"":""#1976d2"",""strokeWeight"":2,""pattern"":""solid""}",[],,-1
multimodal-capabilities,Multimodal Capabilities,"Techniques for processing and generating multiple modalities including vision, audio, video, and text. Covers vision-language models, audio processing, multimodal fusion, and cross-modal learning. Enabling AI systems to understand and interact with diverse data types.","Vision, audio, and cross-modal learning",continent,"[{""lat"":600,""lng"":3800},{""lat"":600,""lng"":4096},{""lat"":1400,""lng"":4096},{""lat"":1400,""lng"":3800}]","{""fillColor"":""#9c27b0"",""fillOpacity"":0.45,""strokeColor"":""#7b1fa2"",""strokeWeight"":2,""pattern"":""solid""}",[],,-1
training-optimization,Training & Optimization,"Methods for efficiently training, fine-tuning, and optimizing language models. Includes pre-training strategies, parameter-efficient fine-tuning, quantization, pruning, and distillation. Focused on reducing computational costs while maintaining or improving model performance.",Efficient training and model optimization,continent,"[{""lat"":1600,""lng"":800},{""lat"":1600,""lng"":2200},{""lat"":2400,""lng"":2200},{""lat"":2400,""lng"":800}]","{""fillColor"":""#ff9800"",""fillOpacity"":0.45,""strokeColor"":""#f57c00"",""strokeWeight"":2,""pattern"":""solid""}",[],,-1
rlhf-archipelago,RLHF Archipelago,"Reinforcement Learning from Human Feedback techniques for aligning language models with human preferences. Includes reward modeling, preference learning, policy optimization, and direct preference optimization. Core methodology for creating helpful and aligned AI systems.",Human feedback alignment techniques,archipelago,"[{""lat"":1600,""lng"":1900},{""lat"":1600,""lng"":2300},{""lat"":1900,""lng"":2300},{""lat"":1900,""lng"":1900}]","{""fillColor"":""#4caf50"",""fillOpacity"":0.55,""strokeColor"":""#388e3c"",""strokeWeight"":2,""pattern"":""solid""}",[],alignment-safety,0
constitutional-ai,Constitutional AI Island,"Self-supervised alignment technique where models critique and revise their own responses based on constitutional principles. Developed by Anthropic, this approach enables AI systems to align themselves with human values through iterative self-improvement and principle-based reasoning.",Self-supervised alignment via principles,island,"[{""lat"":1650,""lng"":1950},{""lat"":1650,""lng"":2050},{""lat"":1750,""lng"":2050},{""lat"":1750,""lng"":1950}]","{""fillColor"":""#4caf50"",""fillOpacity"":0.65,""strokeColor"":""#2e7d32"",""strokeWeight"":2,""pattern"":""solid""}",[],rlhf-archipelago,1
quantization-techniques,Quantization Techniques,"Methods for reducing model size and computational requirements through weight quantization and mixed-precision training. Includes post-training quantization, quantization-aware training, and low-bit representations. Enables deployment of large models on resource-constrained devices.",Model compression via quantization,archipelago,"[{""lat"":1600,""lng"":800},{""lat"":1600,""lng"":1200},{""lat"":1900,""lng"":1200},{""lat"":1900,""lng"":800}]","{""fillColor"":""#ff9800"",""fillOpacity"":0.55,""strokeColor"":""#f57c00"",""strokeWeight"":2,""pattern"":""solid""}",[],training-optimization,0
lora-peft,LoRA & PEFT Methods,Parameter-efficient fine-tuning techniques that adapt large models with minimal additional parameters. LoRA (Low-Rank Adaptation) and other PEFT methods enable fast adaptation to new tasks without full model retraining. Critical for practical deployment and customization of large language models.,Parameter-efficient fine-tuning,archipelago,"[{""lat"":1600,""lng"":1400},{""lat"":1600,""lng"":1800},{""lat"":1900,""lng"":1800},{""lat"":1900,""lng"":1400}]","{""fillColor"":""#ff9800"",""fillOpacity"":0.55,""strokeColor"":""#f57c00"",""strokeWeight"":2,""pattern"":""solid""}",[],training-optimization,0
chain-of-thought,Chain-of-Thought Prompting,"Technique for improving LLM reasoning by encouraging step-by-step problem decomposition and explicit reasoning chains. Demonstrates how models can break down complex problems into manageable steps, leading to more accurate solutions. Foundational for advanced reasoning capabilities.",Step-by-step reasoning prompting,archipelago,"[{""lat"":1600,""lng"":2800},{""lat"":1600,""lng"":3200},{""lat"":1900,""lng"":3200},{""lat"":1900,""lng"":2800}]","{""fillColor"":""#2196f3"",""fillOpacity"":0.55,""strokeColor"":""#1976d2"",""strokeWeight"":2,""pattern"":""solid""}",[],reasoning-planning,0
tool-use-agents,Tool Use & Agents,"Frameworks enabling language models to interact with external tools, APIs, and environments. Includes function calling, action spaces, and multi-step agent loops. Extends LLM capabilities beyond text generation to real-world problem-solving and autonomous task execution.",External tool integration and agents,archipelago,"[{""lat"":1600,""lng"":3400},{""lat"":1600,""lng"":3800},{""lat"":1900,""lng"":3800},{""lat"":1900,""lng"":3400}]","{""fillColor"":""#2196f3"",""fillOpacity"":0.55,""strokeColor"":""#1976d2"",""strokeWeight"":2,""pattern"":""solid""}",[],reasoning-planning,0
vision-language,Vision-Language Models,"Models that jointly process visual and textual information for tasks like image captioning, visual question answering, and scene understanding. Integrates computer vision with language understanding to create systems that can reason about images in natural language.",Image understanding and captioning,archipelago,"[{""lat"":1600,""lng"":3900},{""lat"":1600,""lng"":4096},{""lat"":1900,""lng"":4096},{""lat"":1900,""lng"":3900}]","{""fillColor"":""#9c27b0"",""fillOpacity"":0.55,""strokeColor"":""#7b1fa2"",""strokeWeight"":2,""pattern"":""solid""}",[],multimodal-capabilities,0
in-context-learning,In-Context Learning,"Ability of language models to learn from examples provided in the prompt without parameter updates. Includes few-shot prompting, prompt engineering, and meta-learning. Demonstrates emergent learning capabilities enabling rapid adaptation to new tasks.",Few-shot and prompt-based learning,island,"[{""lat"":1650,""lng"":3450},{""lat"":1650,""lng"":3550},{""lat"":1750,""lng"":3550},{""lat"":1750,""lng"":3450}]","{""fillColor"":""#2196f3"",""fillOpacity"":0.65,""strokeColor"":""#1565c0"",""strokeWeight"":2,""pattern"":""solid""}",[],reasoning-planning,1
model-compression,Model Compression Bay,"Techniques combining pruning, distillation, and quantization for aggressive model size reduction. Creates efficient student models that maintain performance of larger teacher models through knowledge transfer and architectural optimization.",Knowledge distillation and pruning,island,"[{""lat"":1650,""lng"":900},{""lat"":1650,""lng"":1000},{""lat"":1750,""lng"":1000},{""lat"":1750,""lng"":900}]","{""fillColor"":""#ff9800"",""fillOpacity"":0.65,""strokeColor"":""#e65100"",""strokeWeight"":2,""pattern"":""solid""}",[],quantization-techniques,1
retrieval-augmented,Retrieval-Augmented Generation,"Technique combining information retrieval with language generation for improved factuality and knowledge-aware responses. Retrieves relevant documents or knowledge bases before generation, reducing hallucinations and enabling incorporation of external knowledge sources.",Knowledge-augmented text generation,island,"[{""lat"":1650,""lng"":1450},{""lat"":1650,""lng"":1550},{""lat"":1750,""lng"":1550},{""lat"":1750,""lng"":1450}]","{""fillColor"":""#ff9800"",""fillOpacity"":0.65,""strokeColor"":""#e65100"",""strokeWeight"":2,""pattern"":""solid""}",[],lora-peft,1
evaluation-benchmarks,Evaluation & Benchmarks,"Frameworks and datasets for systematically evaluating language model capabilities across diverse dimensions. Includes standardized benchmarks, human evaluation protocols, and automated metrics for assessing reasoning, safety, knowledge, and other LLM properties.",Performance evaluation and benchmarking,island,"[{""lat"":2200,""lng"":2000},{""lat"":2200,""lng"":2100},{""lat"":2300,""lng"":2100},{""lat"":2300,""lng"":2000}]","{""fillColor"":""#00bcd4"",""fillOpacity"":0.65,""strokeColor"":""#0097a7"",""strokeWeight"":2,""pattern"":""solid""}",[],training-optimization,1