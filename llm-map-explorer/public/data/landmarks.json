[
  {
    "id": "lm-001",
    "name": "Attention Is All You Need",
    "type": "paper",
    "year": 2017,
    "organization": "Google DeepMind",
    "authors": [
      "Vaswani",
      "A.; Shazeer",
      "N.; Parmar",
      "N."
    ],
    "description": "Introduced the Transformer architecture with self-attention mechanisms.",
    "abstract": "This paper presents the Transformer, a novel sequence-to-sequence architecture based entirely on attention mechanisms.",
    "externalLinks": [
      {
        "type": "arxiv",
        "url": "https://arxiv.org/abs/1706.03762",
        "label": "arXiv"
      },
      {
        "type": "paper",
        "url": "https://proceedings.neurips.cc/paper/2017/file/",
        "label": "PDF"
      }
    ],
    "coordinates": [
      37.78,
      -122.41
    ],
    "capabilityId": "cap-001",
    "relatedLandmarks": [
      "lm-002",
      "lm-003"
    ],
    "tags": [
      "transformer",
      "attention",
      "sequence-to-sequence",
      "nlp"
    ],
    "icon": "\ud83d\udcc4",
    "metadata": {},
    "zoomThreshold": 0
  },
  {
    "id": "lm-002",
    "name": "GPT-2",
    "type": "model",
    "year": 2019,
    "organization": "OpenAI",
    "authors": [
      "Radford",
      "A.; Wu",
      "J.; Child",
      "R."
    ],
    "description": "Large-scale unsupervised language model with 1.5B parameters.",
    "abstract": "Language models are unsupervised multitask learners. We demonstrate that language models begin learning these tasks without any explicit supervision when trained on a new dataset of Reddit links.",
    "externalLinks": [
      {
        "type": "arxiv",
        "url": "https://arxiv.org/abs/1902.10165",
        "label": "arXiv"
      },
      {
        "type": "github",
        "url": "https://github.com/openai/gpt-2",
        "label": "GitHub"
      }
    ],
    "coordinates": [
      37.79,
      -122.4
    ],
    "capabilityId": "cap-002",
    "relatedLandmarks": [
      "lm-001",
      "lm-003"
    ],
    "tags": [
      "language-model",
      "generation",
      "gpt"
    ],
    "icon": "\ud83e\udd16",
    "metadata": {
      "parameters": "1.5B",
      "architecture": "Transformer",
      "trainingMethod": "Unsupervised pre-training",
      "capabilities": [
        "text-generation",
        "few-shot-learning"
      ],
      "releaseDate": "2019-02-14",
      "license": "MIT"
    },
    "zoomThreshold": 1
  },
  {
    "id": "lm-003",
    "name": "BERT",
    "type": "model",
    "year": 2018,
    "organization": "Google Research",
    "authors": [
      "Devlin",
      "J.; Chang",
      "M.-W.; Lee",
      "K."
    ],
    "description": "Bidirectional Encoder Representations from Transformers for understanding contextual word relationships.",
    "abstract": "We introduce BERT, a new method of pre-training language representations. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text.",
    "externalLinks": [
      {
        "type": "arxiv",
        "url": "https://arxiv.org/abs/1810.04805",
        "label": "arXiv"
      },
      {
        "type": "github",
        "url": "https://github.com/google-research/bert",
        "label": "GitHub"
      }
    ],
    "coordinates": [
      37.8,
      -122.39
    ],
    "capabilityId": "cap-002",
    "relatedLandmarks": [
      "lm-001",
      "lm-002"
    ],
    "tags": [
      "language-model",
      "pretraining",
      "bert",
      "embeddings"
    ],
    "icon": "\ud83e\udd16",
    "metadata": {
      "parameters": "340M",
      "architecture": "Transformer",
      "trainingMethod": "Masked Language Modeling",
      "capabilities": [
        "text-classification",
        "token-classification",
        "semantic-similarity"
      ],
      "releaseDate": "2018-10-11",
      "license": "Apache-2.0"
    },
    "zoomThreshold": 1
  },
  {
    "id": "lm-004",
    "name": "Constitutional AI",
    "type": "paper",
    "year": 2023,
    "organization": "Anthropic",
    "authors": [
      "Bai",
      "Y.; Jones",
      "A.; Ndousse",
      "K."
    ],
    "description": "Harmless and helpful AI through constitutional methods and self-critique.",
    "abstract": "Language models demonstrate remarkable capabilities across a diverse set of tasks, but their proneness to generating harmful or misleading content remains a critical concern.",
    "externalLinks": [
      {
        "type": "arxiv",
        "url": "https://arxiv.org/abs/2212.08073",
        "label": "arXiv"
      }
    ],
    "coordinates": [
      37.81,
      -122.38
    ],
    "capabilityId": "cap-003",
    "relatedLandmarks": [
      "lm-005"
    ],
    "tags": [
      "alignment",
      "safety",
      "constitutional-ai",
      "harmful-content"
    ],
    "icon": "\ud83d\udcc4",
    "metadata": {},
    "zoomThreshold": 1
  },
  {
    "id": "lm-005",
    "name": "GPT-4",
    "type": "model",
    "year": 2023,
    "organization": "OpenAI",
    "authors": [
      "OpenAI Team"
    ],
    "description": "Multimodal large language model with improved reasoning and reduced hallucinations.",
    "abstract": "GPT-4 is a large multimodal model that, while less capable than humans in many real-world scenarios, exhibits human-level performance on various professional and academic benchmarks.",
    "externalLinks": [
      {
        "type": "paper",
        "url": "https://arxiv.org/abs/2303.08774",
        "label": "Technical Report"
      }
    ],
    "coordinates": [
      37.82,
      -122.37
    ],
    "capabilityId": "cap-002",
    "relatedLandmarks": [
      "lm-002",
      "lm-004"
    ],
    "tags": [
      "language-model",
      "gpt",
      "multimodal",
      "frontier"
    ],
    "icon": "\ud83e\udd16",
    "metadata": {
      "parameters": "Unknown",
      "architecture": "Transformer",
      "trainingMethod": "RLHF with constitutional AI",
      "capabilities": [
        "text-generation",
        "vision",
        "code-generation",
        "reasoning"
      ],
      "releaseDate": "2023-03-14",
      "license": "Proprietary"
    },
    "zoomThreshold": 0
  }
]