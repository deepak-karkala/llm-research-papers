[
  {
    "id": "route_aligned_chatbot_llama_style",
    "routeName": "Voyage to an Aligned Chatbot (LLaMA 2 Style)",
    "description": "A common pipeline for developing a helpful and harmless conversational AI, similar to LLaMA 2-Chat.",
    "startPointDescription": "Starting with a strong pre-trained foundation model (LLaMA 2 base).",
    "endPointCapabilityID": "island_advanced_dialogue",
    "stages": [
      {
        "stageID": 1,
        "stageName": "Supervised Fine-Tuning (SFT)",
        "description": "Fine-tune on high-quality instruction-response pairs to teach the model to follow instructions and adopt a chat format.",
        "islandID": "island_sft",
        "shipTransformationDescription": "Ship equipped with 'Instruction Following Sails' and painted for 'Chat Persona'.",
        "shipTransformationIconChange": "add_chat_sails",
        "keyLandmarkIDs": ["landmark_instructgpt_paper"]
      },
      {
        "stageID": 2,
        "stageName": "Reward Modeling",
        "description": "Train a separate model to learn human preferences by ranking different model responses to the same prompt.",
        "islandID": "bay_reward_modeling",
        "shipTransformationDescription": "Ship's navigators (crew) learn to use a 'Preference Compass'.",
        "shipTransformationIconChange": "add_compass_icon",
        "keyLandmarkIDs": []
      },
      {
        "stageID": 3,
        "stageName": "Reinforcement Learning (PPO)",
        "description": "Further fine-tune the SFT model using the reward model as a guide to generate responses that humans prefer, enhancing helpfulness and harmlessness.",
        "islandID": "port_ppo",
        "shipTransformationDescription": "Ship's rudder and helm upgraded for 'Ethical Navigation' and 'Helpful Maneuvers'.",
        "shipTransformationIconChange": "upgrade_helm",
        "keyLandmarkIDs": ["landmark_instructgpt_paper"]
      }
    ]
  },
  {
    "id": "route_reasoning_model_deepseek_style",
    "routeName": "Forging a Reasoning Model (DeepSeek-R1-Zero Style)",
    "description": "A cold-start RL approach to instill reasoning capabilities in a base LLM.",
    "startPointDescription": "Starting with a powerful pre-trained foundation model (e.g., DeepSeek-V3).",
    "endPointCapabilityID": "continent_reasoning",
    "stages": [
      {
        "stageID": 1,
        "stageName": "Reinforcement Learning with Verifiable Rewards (Cold-Start)",
        "description": "Train the base model using RL (e.g., GRPO) with rewards derived from verifiable outcomes (e.g., math problem correctness, code execution). Focus on format and accuracy rewards.",
        "islandID": "island_rl_for_reasoning",
        "shipTransformationDescription": "Ship's internal 'Logic Engine' is activated and analytical tools are installed.",
        "shipTransformationIconChange": "add_gears_icon",
        "keyLandmarkIDs": ["landmark_deepseek_r1_model"]
      },
      {
        "stageID": 2,
        "stageName": "Iterative Refinement (Optional)",
        "description": "Further SFT and RL stages can be applied to polish and enhance the reasoning capabilities, potentially incorporating human preference data for broader alignment.",
        "islandID": "island_advanced_reasoning_techniques",
        "shipTransformationDescription": "Ship receives advanced 'Problem-Solving Charts' and 'Self-Correction Mechanisms'.",
        "shipTransformationIconChange": "add_magnifying_glass",
        "keyLandmarkIDs": []
      }
    ]
  }
]