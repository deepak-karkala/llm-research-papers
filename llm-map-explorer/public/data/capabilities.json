[
  {
    "id": "cap-001",
    "name": "Attention Mechanisms",
    "description": "Study of attention and self-attention patterns in neural networks. Foundation of modern transformer-based architectures.",
    "shortDescription": "Attention & self-attention patterns",
    "level": "continent",
    "polygonCoordinates": [
      [
        37.7749,
        -122.4194
      ],
      [
        37.8749,
        -122.4194
      ],
      [
        37.8749,
        -122.3194
      ],
      [
        37.7749,
        -122.3194
      ]
    ],
    "visualStyleHints": {
      "fillColor": "#FF6B6B",
      "fillOpacity": 0.4,
      "strokeColor": "#FF0000",
      "strokeWeight": 2
    },
    "relatedLandmarks": [
      "lm-001",
      "lm-002"
    ],
    "parentCapabilityId": null,
    "zoomThreshold": 0
  },
  {
    "id": "cap-002",
    "name": "Transformers",
    "description": "Foundation of modern LLMs built on attention mechanisms and encoder-decoder architectures.",
    "shortDescription": "Transformer architectures",
    "level": "archipelago",
    "polygonCoordinates": [
      [
        37.78,
        -122.41
      ],
      [
        37.87,
        -122.41
      ],
      [
        37.87,
        -122.32
      ],
      [
        37.78,
        -122.32
      ]
    ],
    "visualStyleHints": {
      "fillColor": "#4ECDC4",
      "fillOpacity": 0.4,
      "strokeColor": "#0099CC",
      "strokeWeight": 2
    },
    "relatedLandmarks": [
      "lm-001",
      "lm-003"
    ],
    "parentCapabilityId": "cap-001",
    "zoomThreshold": 1
  },
  {
    "id": "cap-003",
    "name": "Alignment & Safety",
    "description": "Techniques for aligning language models with human values and ensuring safe behavior.",
    "shortDescription": "Model alignment & safety techniques",
    "level": "archipelago",
    "polygonCoordinates": [
      [
        37.79,
        -122.42
      ],
      [
        37.88,
        -122.42
      ],
      [
        37.88,
        -122.33
      ],
      [
        37.79,
        -122.33
      ]
    ],
    "visualStyleHints": {
      "fillColor": "#95E1D3",
      "fillOpacity": 0.4,
      "strokeColor": "#00AA00",
      "strokeWeight": 2
    },
    "relatedLandmarks": [
      "lm-004",
      "lm-005"
    ],
    "parentCapabilityId": null,
    "zoomThreshold": 1
  },
  {
    "id": "cap-004",
    "name": "Language Modeling",
    "description": "Core techniques for training large language models on diverse text corpora.",
    "shortDescription": "Language model training techniques",
    "level": "continent",
    "polygonCoordinates": [
      [
        37.8,
        -122.43
      ],
      [
        37.89,
        -122.43
      ],
      [
        37.89,
        -122.34
      ],
      [
        37.8,
        -122.34
      ]
    ],
    "visualStyleHints": {
      "fillColor": "#F38181",
      "fillOpacity": 0.4,
      "strokeColor": "#CC0000",
      "strokeWeight": 2
    },
    "relatedLandmarks": [
      "lm-002",
      "lm-003"
    ],
    "parentCapabilityId": null,
    "zoomThreshold": -1
  }
]